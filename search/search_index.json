{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Parallel ML Docs This organization is from Georgia Tech, HPArch .","title":"Home"},{"location":"#welcome-to-parallel-ml-docs","text":"This organization is from Georgia Tech, HPArch .","title":"Welcome to Parallel ML Docs"},{"location":"asplos2018/real-time/","text":"Introduction This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3]. Installation Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference. Single device (GPU and CPU). (This is NVidia Jetson TX2 version in our paper) Dependencies: tensorflow-gpu >= 1.5.0 Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information Multiple devices (CPU and RPC). (This is Raspberry PI 3 versions in our paper) Dependencies: tensorflow >= 1.5.0 Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt Quick Start Single device (GPU and CPU) (This is NVidia Jetson TX2 version in our paper) GPU Version Execute predict file to run model inference. python predict.py CPU Version CUDA_VISIBLE_DEVICES= python predict.py Multiple devices (CPU and RPC) (This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format. AlexNet For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d VGG16 For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py Refereces [1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"ASPLOS 2018"},{"location":"asplos2018/real-time/#introduction","text":"This is demo of Real-Time Image Recognition Using Collaborative IoT Devices at ACM ReQuEST workshop co-located with ASPLOS 2018 Github repo This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art deep learning neural networks, AlexNet[2] and VGG16[3].","title":"Introduction"},{"location":"asplos2018/real-time/#installation","text":"Please make sure that you have Python 2.7 running on your device. We have two versions of model inference. One is using GPU and running model inference on single machine. Another is using CPU and using RPC to off-shore the computation to other devices. We will have different installation guide for those two versions model inference.","title":"Installation"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu","text":"(This is NVidia Jetson TX2 version in our paper) Dependencies: tensorflow-gpu >= 1.5.0 Keras >= 2.1.3 pip install keras Please refer to official installation guideline from Keras for more information","title":"Single device (GPU and CPU)."},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc","text":"(This is Raspberry PI 3 versions in our paper) Dependencies: tensorflow >= 1.5.0 Keras >= 2.1.3 * avro >= 1.8.2 We have provided dependency file here. You can execute this file to install packages. pip install -r requirements.txt","title":"Multiple devices (CPU and RPC)."},{"location":"asplos2018/real-time/#quick-start","text":"","title":"Quick Start"},{"location":"asplos2018/real-time/#single-device-gpu-and-cpu_1","text":"(This is NVidia Jetson TX2 version in our paper)","title":"Single device (GPU and CPU)"},{"location":"asplos2018/real-time/#gpu-version","text":"Execute predict file to run model inference. python predict.py","title":"GPU Version"},{"location":"asplos2018/real-time/#cpu-version","text":"CUDA_VISIBLE_DEVICES= python predict.py","title":"CPU Version"},{"location":"asplos2018/real-time/#multiple-devices-cpu-and-rpc_1","text":"(This is Raspberry PI 3 versions in our paper) We make a checklist for you before running our program. - [ ] Have all correct packages installed on Raspberry Pi. - [ ] The Raspberry PI has port 12345, 9999 open. - [ ] Put correct IP address in IP table file mutiple-devices/alexnet/resource/ip . The IP table file is in json format.","title":"Multiple devices (CPU and RPC)"},{"location":"asplos2018/real-time/#alexnet","text":"For AlexNet, we have same model partition, so we will use the same node file for different system setup. The IP table is default to 4 devices setup. You need to add 1 more IP address to block1 if you want to test 6 devices setup. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py If you modify our code, you can use flag to debug. python node.py -d","title":"AlexNet"},{"location":"asplos2018/real-time/#vgg16","text":"For VGG16, we have different model separation for different system setup, so we put two directories under mutiple-devices/vgg16 . For 8devices , you should have 3 devices for block234 and 2 devices for fc1 , which means you need 2 IP addresses for those 2 blocks in IP table. For 11devices , you should have 7 devices for block12345 , so put 7 IP addresses at IP table. On all of your device except the initial sender, run the node. python node.py Start the data sender. You should be able to see console log. python initial.py","title":"VGG16"},{"location":"asplos2018/real-time/#refereces","text":"[1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138. [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012. [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.","title":"Refereces"},{"location":"camera/picamera/","text":"Using the PiCamera Module For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12) Visualizing pictures via ssh If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Using the PiCamera"},{"location":"camera/picamera/#using-the-picamera-module","text":"For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with: sudo apt-get update and sudo apt-get upgrade That may take some short time. Then, you have to enable the camera in the raspberry settings by: 'sudo raspi-config' Reboot the pi after that: sudo reboot Give some time for the pi to reboot and log again into it. Finally, test your camera with: raspistill -o image.jpg (it takes a picture and saves it as image.jpg ) After that, a new jpeg file should appear in your directory. General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12)","title":"Using the PiCamera Module"},{"location":"camera/picamera/#visualizing-pictures-via-ssh","text":"If you want to display the images of image files in your computer, make sure to connect to the pi with: ssh -X pi@<pi_adress> (focus on the -X ) Install feh if you the don't have it yet: sudo apt-get install feh And finally do: feh <image_file>.jpg","title":"Visualizing pictures via ssh"},{"location":"camera/webcam/","text":"Webcam Video/Image Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam video/image"},{"location":"camera/webcam/#webcam-videoimage","text":"Install streamer: sudo apt-get install streamer Record video or capture streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg streamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format) ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov","title":"Webcam Video/Image"},{"location":"getting-started/setting-up-pi/","text":"Pre-Requisite Raspberry Pi can ONLY be accessed from lab's network. Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ . Connect to Raspberry Pi Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command. Python Setup Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/","title":"Connecting to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#pre-requisite","text":"Raspberry Pi can ONLY be accessed from lab's network. Connect to NETGEAR79 Wi-Fi with password 78zBJr!4bVdpaFIQ .","title":"Pre-Requisite"},{"location":"getting-started/setting-up-pi/#connect-to-raspberry-pi","text":"Find IP of available Pi Go to 192.168.1.1 in your browser with username admin and password password to check IP of all connected Raspberry Pis. Under connected devices tab, you will be able to see the IP of all connected Raspberry Pis. Connect through SSH ssh pi@<ip> Use password raspberry for connection or for sudo command.","title":"Connect to Raspberry Pi"},{"location":"getting-started/setting-up-pi/#python-setup","text":"Python Environment We are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING. Package Installation pip is always recommended for Python package installations. A cleaner way would be using virtualenv , so that your environment won't interfere with others'. Long compling time with pip installation You can use piwheels by placing the following lines in /etc/pip.conf: [global] extra-index-url=https://www.piwheels.org/simple Then pip will search in wheels to install any package first. Also, you can download your wheel from here manually: https://pythonwheels.com/","title":"Python Setup"},{"location":"irobot/keyboard/","text":"Control with Keyboard Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Keyboard Control"},{"location":"irobot/keyboard/#control-with-keyboard","text":"Make sure Pi is connected to power and serial ssh -X pi@192.168.1.2 sudo chmod o+rw /dev/ttyUSB0 gtkterm Configure port to USB0 Change baud rate to 115200 python create2_cmds.py Click connect and type /dev/ttyUSB0 (the above) Press 'p' then 'f' The robot is now controllable To see data from sensors (power): - View -> Hexadecimal - Log -> to somelogfile.txt - Go to python and press 'z' to begin log stream - Do whatever (ML stuff) - Stop logging from log menu when done - translatorStream.py -> change file read name to somelogfile.txt (whatever name) - python translatorStream.py - done.txt contains voltage and current values","title":"Control with Keyboard"},{"location":"mapping/lidar-slam/","text":"Template - Lidar SLAM","title":"Lidar SLAM"},{"location":"mapping/lidar-slam/#template-lidar-slam","text":"","title":"Template - Lidar SLAM"},{"location":"people/people/","text":"Our Group Faculty * Hyesoon Kim Graduate Students Ramyad Hadidi Jiashen Cao Undergraduate Students Fall 2017 Jiashen Cao Matthew Woodward Spring 2018 Jiashen Cao Fall 2018 Chunjun Jia Spring 2019 Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia Summer 2019 Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"People"},{"location":"people/people/#our-group","text":"","title":"Our Group"},{"location":"people/people/#faculty","text":"","title":"Faculty"},{"location":"people/people/#hyesoon-kim","text":"","title":"* Hyesoon Kim"},{"location":"people/people/#graduate-students","text":"Ramyad Hadidi Jiashen Cao","title":"Graduate Students"},{"location":"people/people/#undergraduate-students","text":"","title":"Undergraduate Students"},{"location":"people/people/#fall-2017","text":"Jiashen Cao Matthew Woodward","title":"Fall 2017"},{"location":"people/people/#spring-2018","text":"Jiashen Cao","title":"Spring 2018"},{"location":"people/people/#fall-2018","text":"Chunjun Jia","title":"Fall 2018"},{"location":"people/people/#spring-2019","text":"Matthew Merck Arthur Siqueira Qiusen Huang Abhijeet Saraha Bingyao Wang Dongsuk Lim Lixing Liu Chunjun Jia","title":"Spring 2019"},{"location":"people/people/#summer-2019","text":"Arthur Siqueira Abhijeet Saraha Chunjun Jia Taejoon Park Mohan Dodda Sayuj Shajith Songming Liu Thai Tran Jinwoo Park Nima Shoghi Younmin Bae Akanksha Telagamsetty Ayushi Chaudhary Abhi Bothera Kabir Kohli","title":"Summer 2019"},{"location":"speech/deepspeech/","text":"Deepspeech on Raspberry Pi Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package Run Deepspeech with Trained Model (use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download Making Your Own Model Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Deepspeech"},{"location":"speech/deepspeech/#deepspeech-on-raspberry-pi","text":"Requirements: have python3 installed with pip3 https://github.com/mozilla/DeepSpeech#using-the-python-package","title":"Deepspeech on Raspberry Pi"},{"location":"speech/deepspeech/#run-deepspeech-with-trained-model","text":"(use python deepspeech package) WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are: Make a virtual environment: Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version virtualenv -p python3 $HOME/tmp/deepspeech-venv/ Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made deepspeech-venv will be the name of the environment so change that if you want a different name Or just make a virtualenv how you normally do Activate the virtual environment Now the virtual environment is created with a bin folder with activate document source $HOME/tmp/deepspeech-venv/bin/activate This creates a virtual environment where you can install deepspeech related dependencies Now install deepspeech package on your local environment pip3 install deepspeech Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to) Linux: run this command in the directory you want to put the file: wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory Then, unzip the file using tar command tar xvfz deepspeech-0.5.0-models.tar.gz This creates a folder, called deepspeech-0.5.0-models Now download an audio file you want the model to do speech to text recognition Put this model in the preferred directory Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav EXCEPT: replace my_audio_file.wav with your audio file and --lm and --trie tags are optional Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download","title":"Run Deepspeech with Trained Model"},{"location":"speech/deepspeech/#making-your-own-model","text":"Next we tried to make our own model to see if we can reduce the model size: 1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough 2.) If you want to use a GPU, follow directions from the gpu slack channel for conection Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model: Make or activate your virtualenv for deepspeech Git clone DeepSpeech from the github git clone https://github.com/mozilla/DeepSpeech.git Install required dependencies from requirements.txt file, Run these commands cd deepspeech pip3 install -r requirements.txt If you are using gpu, use tensorflow gpu: pip3 uninstall tensorflow pip3 install 'tensorflow-gpu==1.13.1' Download voice training data from common voice: https://voice.mozilla.org/en/datasets; - Download the Tatoeba dataset - Go to the link, scroll down to the Tatoeba dataset, press more, and press download - Move it to your preferrred directory - Unzip the file The data is needs to be converted wav files. The data needs to be split into train, test, and dev data 3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript - Use import.py and untilA.csv to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located) - Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder - Install pydub (pydub will help convert MP3 to WAV) pip3 install pydub - (Optional) apt-get install ffmpeg - Edit import.py before you start running the code - Change the fullpath variable to the directory that has the audio files - For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019 - Now, run import.py by python3 import.py - As a result, you will have the following files: new_names.csv train.csv dev.csv test.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories - Using ./Deepspeech.py to create your own model ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv","title":"Making Your Own Model"},{"location":"speech/sphinx/","text":"CMU Sphinx I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: https://cmusphinx.github.io/wiki/tutorialpocketsphinx/#installation-on-unix-system Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic, which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can also change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary and language model using a tool I found online (link below), specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file. (Dict and LM tool): http://www.speech.cs.cmu.edu/tools/lmtool-new.html","title":"Sphinx"},{"location":"speech/sphinx/#cmu-sphinx","text":"I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: https://cmusphinx.github.io/wiki/tutorialpocketsphinx/#installation-on-unix-system Then I used the pocketsphinx_continuous command line command. There are multiple options, such as -inmic, which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the -infile flag, then type the directory of the file relative to where you are calling the command from. You can also change the dictionary and the language model that the program uses by using the -dict and -lm flags. I created my own dictionary and language model using a tool I found online (link below), specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility. The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate. pi@n1:/Research$ ./decode.out MOVE DOWN MOVE UP TURN TO ME Time Elapsed: 2.049368 pi@n1:/Research$ ./decode.out uh got caught move up learn to make Time Elapsed: 2.049368 Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file. (Dict and LM tool): http://www.speech.cs.cmu.edu/tools/lmtool-new.html","title":"CMU Sphinx"},{"location":"speech/text-to-speech/","text":"Template - Text to Speech","title":"Text to Speech"},{"location":"speech/text-to-speech/#template-text-to-speech","text":"","title":"Template - Text to Speech"},{"location":"vision/character/","text":"Template - Character Recognition","title":"Character Recognition"},{"location":"vision/character/#template-character-recognition","text":"","title":"Template - Character Recognition"}]}